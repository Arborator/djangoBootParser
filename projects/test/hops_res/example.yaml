# Layer dimensions
mlp_input: 1024
mlp_tag_hidden: 16
mlp_arc_hidden: 512
mlp_lab_hidden: 128
# Lexers
lexers:
  - name: word_embeddings
    type: words
    embedding_size: 256
    word_dropout: 0.5
  - name: char_level_embeddings
    type: chars_rnn
    embedding_size: 64
    lstm_output_size: 128
  - name: fasttext
    type: fasttext
  - name: bert-base-multilingual-uncased
    type: bert
    model: "bert-base-multilingual-uncased"
    layers: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
    subwords_reduction: "mean"
# Training hyperparameters
encoder_dropout: 0.5
mlp_dropout: 0.5
batch_size: 8
epochs: 60
lr:
  base: 0.00003
  schedule:
    shape: linear
    warmup_steps: 100